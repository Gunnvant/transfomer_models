{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a161f3e-9694-4d65-9552-f78a7dcd1582",
   "metadata": {},
   "source": [
    "----------------------\n",
    "**Author**: Gunnvant\n",
    "\n",
    "**Description**: Do relevant data manipulations using hugging face datasets and model training flow\n",
    "\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73edca99-697d-4b59-93e5-13c943362ad5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gunnvantsaini/miniforge3/envs/huggingface/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a4bae3e-2aec-4bab-a8aa-ca6b363f7cae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_data = load_dataset(\"csv\",data_files=\"../preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cc893fe-ca33-49f9-a0a1-813d0b97bcf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Word': \"['Thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'London', 'to', 'protest', 'the', 'war', 'in', 'Iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'British', 'troops', 'from', 'that', 'country', '.']\",\n",
       " 'ner_tags': '[11, 11, 11, 11, 11, 11, 7, 11, 11, 11, 11, 11, 7, 11, 11, 11, 11, 11, 16, 11, 11, 11, 11, 11]'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d685b9b-7fb4-4c3d-ab1a-9b1e7dd18396",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"tag_mapping.json\",\"r\") as f:\n",
    "    label2id = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ba1ea89-9af2-4686-b846-1510fff5265c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c55262d-0634-47f1-ba4c-cfbf01c514fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "example = raw_data['train'][0]\n",
    "tokenized_input = tokenizer(ast.literal_eval(example[\"Word\"]), is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcd15e9f-56bc-4fb3-a84b-335ccacbb9ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 5190, 1997, 28337, 2031, 9847, 2083, 2414, 2000, 6186, 1996, 2162, 1999, 5712, 1998, 5157, 1996, 10534, 1997, 2329, 3629, 2013, 2008, 2406, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4faeb61-b020-44ca-8a88-4c1a1b65144b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'thousands',\n",
       " 'of',\n",
       " 'demonstrators',\n",
       " 'have',\n",
       " 'marched',\n",
       " 'through',\n",
       " 'london',\n",
       " 'to',\n",
       " 'protest',\n",
       " 'the',\n",
       " 'war',\n",
       " 'in',\n",
       " 'iraq',\n",
       " 'and',\n",
       " 'demand',\n",
       " 'the',\n",
       " 'withdrawal',\n",
       " 'of',\n",
       " 'british',\n",
       " 'troops',\n",
       " 'from',\n",
       " 'that',\n",
       " 'country',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenized_input['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92e559c7-ac41-4feb-aba0-fdeef51b0b31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Need to treat each word and ner tag entry in data as a list\n",
    "def convert_list(example):\n",
    "    example['Word'] = ast.literal_eval(example['Word'])\n",
    "    example['ner_tags'] = ast.literal_eval(example['ner_tags'])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "218abe4b-7364-4996-9f90-604e30675ce5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_data = raw_data.map(convert_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fb3129e-68b7-42cf-8db1-137c921802c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Word': ['Thousands',\n",
       "  'of',\n",
       "  'demonstrators',\n",
       "  'have',\n",
       "  'marched',\n",
       "  'through',\n",
       "  'London',\n",
       "  'to',\n",
       "  'protest',\n",
       "  'the',\n",
       "  'war',\n",
       "  'in',\n",
       "  'Iraq',\n",
       "  'and',\n",
       "  'demand',\n",
       "  'the',\n",
       "  'withdrawal',\n",
       "  'of',\n",
       "  'British',\n",
       "  'troops',\n",
       "  'from',\n",
       "  'that',\n",
       "  'country',\n",
       "  '.'],\n",
       " 'ner_tags': [11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  7,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  7,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  16,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8346d4c6-eae5-4a0b-9109-2d15d6f006db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'thousands',\n",
       " 'of',\n",
       " 'demonstrators',\n",
       " 'have',\n",
       " 'marched',\n",
       " 'through',\n",
       " 'london',\n",
       " 'to',\n",
       " 'protest',\n",
       " 'the',\n",
       " 'war',\n",
       " 'in',\n",
       " 'iraq',\n",
       " 'and',\n",
       " 'demand',\n",
       " 'the',\n",
       " 'withdrawal',\n",
       " 'of',\n",
       " 'british',\n",
       " 'troops',\n",
       " 'from',\n",
       " 'that',\n",
       " 'country',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Repeat the preprocessing step again on a single example\n",
    "example = raw_data['train'][0]\n",
    "tokenized_input = tokenizer(example[\"Word\"], is_split_into_words=True)\n",
    "tokenizer.convert_ids_to_tokens(tokenized_input['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "323f361f-ad3a-4124-9d4f-f726e9d87233",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### tokenization and alognment logic\n",
    "examples = raw_data['train'][0:10]\n",
    "tokenized_inputs = tokenizer(examples[\"Word\"], truncation=True, is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95ee6c62-6e15-4567-8b0b-f7092026668b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'helicopter',\n",
       " 'guns',\n",
       " '##hips',\n",
       " 'saturday',\n",
       " 'pounded',\n",
       " 'militant',\n",
       " 'hideout',\n",
       " '##s',\n",
       " 'in',\n",
       " 'the',\n",
       " 'or',\n",
       " '##ak',\n",
       " '##zai',\n",
       " 'tribal',\n",
       " 'region',\n",
       " ',',\n",
       " 'where',\n",
       " 'many',\n",
       " 'taliban',\n",
       " 'militants',\n",
       " 'are',\n",
       " 'believed',\n",
       " 'to',\n",
       " 'have',\n",
       " 'fled',\n",
       " 'to',\n",
       " 'avoid',\n",
       " 'an',\n",
       " 'earlier',\n",
       " 'military',\n",
       " 'offensive',\n",
       " 'in',\n",
       " 'nearby',\n",
       " 'south',\n",
       " 'wa',\n",
       " '##zi',\n",
       " '##rist',\n",
       " '##an',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenized_inputs['input_ids'][2]) ### guns and ##hips are one word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fc07544-0ac6-4d31-8c88-e7c78bd6439a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 31,\n",
       " None]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_inputs.word_ids(batch_index=2) ## this is able to find out parts of words for single word (1,1 refers to gunship)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01352656-4ba3-4a7e-ae74-220b7d9b1747",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  7,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  7,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  16,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11],\n",
       " [16,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  12,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  8,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples['ner_tags'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5fede61-765a-46eb-b972-80a8c508ac00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Label Alignment \n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"Word\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5c2b5d9-74b7-49a3-87aa-687e6151405a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_data = raw_data.map(tokenize_and_align_labels,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c466f9dc-aa85-4a68-91fc-d22f600d610f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Word': ['Helicopter',\n",
       "  'gunships',\n",
       "  'Saturday',\n",
       "  'pounded',\n",
       "  'militant',\n",
       "  'hideouts',\n",
       "  'in',\n",
       "  'the',\n",
       "  'Orakzai',\n",
       "  'tribal',\n",
       "  'region',\n",
       "  ',',\n",
       "  'where',\n",
       "  'many',\n",
       "  'Taliban',\n",
       "  'militants',\n",
       "  'are',\n",
       "  'believed',\n",
       "  'to',\n",
       "  'have',\n",
       "  'fled',\n",
       "  'to',\n",
       "  'avoid',\n",
       "  'an',\n",
       "  'earlier',\n",
       "  'military',\n",
       "  'offensive',\n",
       "  'in',\n",
       "  'nearby',\n",
       "  'South',\n",
       "  'Waziristan',\n",
       "  '.'],\n",
       " 'ner_tags': [11,\n",
       "  11,\n",
       "  12,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  7,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  8,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  7,\n",
       "  6,\n",
       "  11],\n",
       " 'input_ids': [101,\n",
       "  7739,\n",
       "  4409,\n",
       "  19801,\n",
       "  5095,\n",
       "  13750,\n",
       "  16830,\n",
       "  29588,\n",
       "  2015,\n",
       "  1999,\n",
       "  1996,\n",
       "  2030,\n",
       "  4817,\n",
       "  25290,\n",
       "  8807,\n",
       "  2555,\n",
       "  1010,\n",
       "  2073,\n",
       "  2116,\n",
       "  16597,\n",
       "  17671,\n",
       "  2024,\n",
       "  3373,\n",
       "  2000,\n",
       "  2031,\n",
       "  6783,\n",
       "  2000,\n",
       "  4468,\n",
       "  2019,\n",
       "  3041,\n",
       "  2510,\n",
       "  5805,\n",
       "  1999,\n",
       "  3518,\n",
       "  2148,\n",
       "  11333,\n",
       "  5831,\n",
       "  15061,\n",
       "  2319,\n",
       "  1012,\n",
       "  102],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  11,\n",
       "  11,\n",
       "  -100,\n",
       "  12,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  -100,\n",
       "  11,\n",
       "  11,\n",
       "  7,\n",
       "  -100,\n",
       "  -100,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  8,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  7,\n",
       "  6,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  11,\n",
       "  -100]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['train'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1b51285-9e5f-4fcf-a9b2-189abd99db0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85563b46-d436-4144-87c2-c4d15e5dfe95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25f23d3e-c3a2-4560-b33b-b60d7edb1f25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|████████████████████████████████| 6.34k/6.34k [00:00<00:00, 6.97MB/s]\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22765f3d-347b-4c0a-96f8-4ce07cb44171",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I-org': 0,\n",
       " 'I-art': 1,\n",
       " 'I-gpe': 2,\n",
       " 'B-art': 3,\n",
       " 'I-nat': 4,\n",
       " 'I-tim': 5,\n",
       " 'I-geo': 6,\n",
       " 'B-geo': 7,\n",
       " 'B-org': 8,\n",
       " 'B-nat': 9,\n",
       " 'B-eve': 10,\n",
       " 'O': 11,\n",
       " 'B-tim': 12,\n",
       " 'I-per': 13,\n",
       " 'I-eve': 14,\n",
       " 'B-per': 15,\n",
       " 'B-gpe': 16}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19c8972c-ee5e-4613-97ba-7c3911fcbf92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_list = list(label2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da41e5e9-afbd-4f2a-b2aa-2f2d46e94374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8866e415-1f9a-43be-8f8a-723d8f32708e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id2label = {k:v for v,k in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a51b080-3f38-42f4-8a11-c9a3e61c4c88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=17, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6215a6f3-31f5-4f6a-9a27-6053a238bf6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_ner_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=raw_data[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3041d370-6aca-4da7-8ddc-5b3dd8d73527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
