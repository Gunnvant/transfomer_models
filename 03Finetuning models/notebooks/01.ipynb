{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d9b214b-8b38-4f66-a96b-75c8bb69b640",
   "metadata": {
    "tags": []
   },
   "source": [
    "------------\n",
    "\n",
    "**Author**: Gunnvant\n",
    "\n",
    "**Description**: Explore the `datasets` library\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa777073-7ff7-4914-9778-74ac5db662c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d044d029-85f0-4fde-bb96-64c11ea37414",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|████████████████████████████████████████████████| 28.8k/28.8k [00:00<00:00, 14.9MB/s]\n",
      "Downloading metadata: 100%|██████████████████████████████████████████████████████| 28.7k/28.7k [00:00<00:00, 19.0MB/s]\n",
      "Downloading readme: 100%|████████████████████████████████████████████████████████| 27.9k/27.9k [00:00<00:00, 10.5MB/s]\n",
      "Downloading data files:   0%|                                                                   | 0/3 [00:00<?, ?it/s]\n",
      "Downloading data: 6.22kB [00:00, 4.56MB/s]\n",
      "Downloading data files:  33%|███████████████████▋                                       | 1/3 [00:00<00:00,  2.18it/s]\n",
      "Downloading data: 0.00B [00:00, ?B/s]\u001b[A\n",
      "Downloading data: 1.05MB [00:00, 7.42MB/s][A\n",
      "Downloading data files:  67%|███████████████████████████████████████▎                   | 2/3 [00:00<00:00,  2.33it/s]\n",
      "Downloading data: 441kB [00:00, 4.73MB/s]\n",
      "Downloading data files: 100%|███████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.47it/s]\n",
      "Generating train split: 100%|███████████████████████████████████████████| 3668/3668 [00:00<00:00, 18744.21 examples/s]\n",
      "Generating validation split: 100%|█████████████████████████████████████████| 408/408 [00:00<00:00, 4960.21 examples/s]\n",
      "Generating test split: 100%|████████████████████████████████████████████| 1725/1725 [00:00<00:00, 36797.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"glue\",\"mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bc7aff8-20fb-42b5-81f9-6536f0f43fef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75613913-3133-48ee-af05-64b966bca627",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_datasets_train = raw_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2769c5f8-ca42-4998-a7ff-2e760c83ede7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9276671-2cfd-474f-ac5a-c806ed1b23e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': ['Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .',\n",
       "  'The stock rose $ 2.11 , or about 11 percent , to close Friday at $ 21.51 on the New York Stock Exchange .',\n",
       "  'Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier .',\n",
       "  'The Nasdaq had a weekly gain of 17.27 , or 1.2 percent , closing at 1,520.15 on Friday .',\n",
       "  'The DVD-CCA then appealed to the state Supreme Court .'],\n",
       " 'sentence2': ['Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .',\n",
       "  'PG & E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .',\n",
       "  \"With the scandal hanging over Stewart 's company , revenue the first quarter of the year dropped 15 percent from the same period a year earlier .\",\n",
       "  'The tech-laced Nasdaq Composite .IXIC rallied 30.46 points , or 2.04 percent , to 1,520.15 .',\n",
       "  'The DVD CCA appealed that decision to the U.S. Supreme Court .'],\n",
       " 'label': [0, 1, 1, 0, 1],\n",
       " 'idx': [3, 4, 5, 6, 7]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets_train[3:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ea92a4b-1994-4c13-9ff7-937b0b1240e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets_train.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e960ecbc-c2d6-4a19-9158-c40761450066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "ckpt = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "058513d4-9add-40ee-9704-7cd532570dac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|█████████████████████████████████████████| 28.0/28.0 [00:00<00:00, 45.9kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|███████████████████████████████████████████| 570/570 [00:00<00:00, 2.27MB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████████████████████████████████████| 232k/232k [00:00<00:00, 599kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|█████████████████████████████████████████| 466k/466k [00:00<00:00, 15.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca11f3d-5769-4423-8603-3fe80819d708",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tokenize the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b6f5db3-419b-40bb-9915-7dff5db63c42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aed546ca-9139-49bd-994e-a0a743c81dd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d778303-4953-48a3-9889-6b8612c33d86",
   "metadata": {},
   "source": [
    "The `tokenized_dataset` is now in the memory. During model training we may not want to do this and stream data on demand to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cc52bad-c9d6-4365-9b43-f13439bb4828",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 9805,\n",
       " 3540,\n",
       " 11514,\n",
       " 2050,\n",
       " 3079,\n",
       " 11282,\n",
       " 2243,\n",
       " 1005,\n",
       " 1055,\n",
       " 2077,\n",
       " 4855,\n",
       " 1996,\n",
       " 4677,\n",
       " 2000,\n",
       " 3647,\n",
       " 4576,\n",
       " 1999,\n",
       " 2687,\n",
       " 2005,\n",
       " 1002,\n",
       " 1016,\n",
       " 1012,\n",
       " 1019,\n",
       " 4551,\n",
       " 1012,\n",
       " 102,\n",
       " 9805,\n",
       " 3540,\n",
       " 11514,\n",
       " 2050,\n",
       " 4149,\n",
       " 11282,\n",
       " 2243,\n",
       " 1005,\n",
       " 1055,\n",
       " 1999,\n",
       " 2786,\n",
       " 2005,\n",
       " 1002,\n",
       " 6353,\n",
       " 2509,\n",
       " 2454,\n",
       " 1998,\n",
       " 2853,\n",
       " 2009,\n",
       " 2000,\n",
       " 3647,\n",
       " 4576,\n",
       " 2005,\n",
       " 1002,\n",
       " 1015,\n",
       " 1012,\n",
       " 1022,\n",
       " 4551,\n",
       " 1999,\n",
       " 2687,\n",
       " 1012,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['input_ids'][1] ## Notice that the inputs are padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb72570b-392c-4c08-8860-57681601b9a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(i) for i in tokenized_dataset['input_ids']][0:20]## length of all the batches is same and has been fixed to be equal to the longest "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4c71c0-8e6f-462d-8f3b-71ed7890508e",
   "metadata": {},
   "source": [
    "Create a training dataset using the `dataset` library and the `map` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97643ad6-03f6-43cc-b3a8-5e864a46d2cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fad74867-50d1-4f87-9b16-edfc40098af2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████| 3668/3668 [00:00<00:00, 4996.59 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████| 408/408 [00:00<00:00, 4959.04 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████| 1725/1725 [00:00<00:00, 5293.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = raw_datasets.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "054cb343-312e-4221-bae1-2d25d38ab23a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset ## this is different from the earlier operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be19888e-b0c2-4a07-b380-e317f352eb32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = tokenized_dataset['train'][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28d4ba9f-69de-4ffa-941b-f447bd9a2a14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50,\n",
       " 59,\n",
       " 47,\n",
       " 67,\n",
       " 59,\n",
       " 50,\n",
       " 62,\n",
       " 32,\n",
       " 45,\n",
       " 60,\n",
       " 51,\n",
       " 47,\n",
       " 42,\n",
       " 61,\n",
       " 53,\n",
       " 44,\n",
       " 53,\n",
       " 79,\n",
       " 57,\n",
       " 70,\n",
       " 63,\n",
       " 35,\n",
       " 54,\n",
       " 64,\n",
       " 52,\n",
       " 47,\n",
       " 68,\n",
       " 58,\n",
       " 60,\n",
       " 35,\n",
       " 43,\n",
       " 34,\n",
       " 48,\n",
       " 65,\n",
       " 27,\n",
       " 73,\n",
       " 31,\n",
       " 50,\n",
       " 36,\n",
       " 61,\n",
       " 57,\n",
       " 54,\n",
       " 41,\n",
       " 64,\n",
       " 53,\n",
       " 38,\n",
       " 68,\n",
       " 45,\n",
       " 57,\n",
       " 39,\n",
       " 36,\n",
       " 68,\n",
       " 63,\n",
       " 47,\n",
       " 37,\n",
       " 62,\n",
       " 59,\n",
       " 58,\n",
       " 50,\n",
       " 33,\n",
       " 61,\n",
       " 34,\n",
       " 71,\n",
       " 64,\n",
       " 74,\n",
       " 30,\n",
       " 54,\n",
       " 53,\n",
       " 72,\n",
       " 70,\n",
       " 44,\n",
       " 58,\n",
       " 78,\n",
       " 40,\n",
       " 60,\n",
       " 50,\n",
       " 55,\n",
       " 31,\n",
       " 62,\n",
       " 46,\n",
       " 58,\n",
       " 70,\n",
       " 49,\n",
       " 49,\n",
       " 42,\n",
       " 34,\n",
       " 70,\n",
       " 50,\n",
       " 34,\n",
       " 65,\n",
       " 49,\n",
       " 39,\n",
       " 53,\n",
       " 37,\n",
       " 28,\n",
       " 70,\n",
       " 66,\n",
       " 68,\n",
       " 62,\n",
       " 62]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(i) for i in batch['input_ids']] ## all the inputs in one batch have a different length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0cf0d1-e8e4-4803-b84d-5c718186ac00",
   "metadata": {},
   "source": [
    "### Dynamic Batching\n",
    "\n",
    "To make sure that each batch has the same size for all the inputs we will use a `collate function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "463ad92a-189d-4c5c-95d1-8a3d9bbbd239",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dffe62d7-e69c-45c9-bc35-f846849eed87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = tokenized_dataset[\"train\"][:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd77bb9e-dc75-4dda-9f3c-cef659b824b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "batch = data_collator(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "011a8f5a-4f0a-4e0a-85da-c2f5a39d93f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[67, 67, 67, 67, 67, 67, 67, 67]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(i) for i in batch['input_ids']] ## All the inputs in the batch have same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1500d32-ba1f-4e9b-9165-d9e8615cb1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
