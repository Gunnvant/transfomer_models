{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b6827bc-09ed-4cb1-86eb-87cd3cd8f31b",
   "metadata": {},
   "source": [
    "--------------------\n",
    "**Author**: Gunnvant\n",
    "\n",
    "**Description**: Self attention notes\n",
    "\n",
    "--------------------\n",
    "\n",
    "## Key, Query and Value Matrix\n",
    "\n",
    "Watch [this](https://www.youtube.com/watch?v=H-4bmOxiKyU) video. \n",
    "\n",
    "**TLDR**:\n",
    "\n",
    "- Simple word embeddings don't take into account the local context of all the words in a given chunk\n",
    "- Query, Q will have dim $N\\times D_q$\n",
    "- Key, K will have dim $N\\times D_k$\n",
    "- Value, V will have dim $N\\times D_v$ \n",
    "\n",
    "Following transform is computed:\n",
    "\n",
    "$Attention = softmax(\\frac{QK^T}{\\sqrt(d_k)})V$\n",
    "\n",
    "$QK^T$, represents the similarity of each word with all the others\n",
    "\n",
    "$softmax(\\frac{QK^T}{\\sqrt(d_k)})$, normalizes the similarities and gives a weight term\n",
    "\n",
    "$softmax(\\frac{QK^T}{\\sqrt(d_k)})V$, provides a weighted average of word representations and how similar they are to each other in a given input chunk.\n",
    "\n",
    "\n",
    "## Basic Transformers block and training flow in torch\n",
    "\n",
    "This [video](https://www.youtube.com/watch?v=ISNdQcPhsts&t=9693s) is the reference\n",
    "\n",
    "![](transformers.png)\n",
    "\n",
    "### Coding the embedding layer:\n",
    "\n",
    "- Convert the tokens to input ids\n",
    "- Map the input ids to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "649bde77-6acb-455d-90fe-e3f689998940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self,dim_model,vocab_size):\n",
    "        super().__init__()\n",
    "        self.dim_model=dim_model\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size,dim_model)\n",
    "    def forward(self,x):\n",
    "        return self.embedding(x)* math.sqrt(self.dim_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc942136-808f-46d3-903e-326a8b20566a",
   "metadata": {},
   "source": [
    "### Coding positional encodings\n",
    "\n",
    "- Add positional encodings to the word embeddings\n",
    "- For even indices it is computed as $PE(pos,2i) = sin(\\frac{pos}{10000^\\frac{2i}{d_model}})$\n",
    "- For odd indices it is computed as $PE(pos,2i+1) = cos(\\frac{pos}{10000^\\frac{2i}{d_model}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aed4d6f1-8df1-4b35-a304-86e4200714c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "d_model = 5\n",
    "seq_len = 2 ## also the poistions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8f1df5-c4a4-4053-ae08-f62063aabf02",
   "metadata": {},
   "source": [
    "Imagine we have two words that are passed, then each word will be represented by a vector of dim 5. Word 1 is at position one in the seq and word2 is at position two. For word 1 following will be the PE:\n",
    "\n",
    "$cos(\\frac{0}{10000^\\frac{2*0}{5}}),cos(\\frac{0}{10000^\\frac{2*1}{5}}),sin(\\frac{0}{10000^\\frac{2*2}{4}}),cos(\\frac{0}{10000^\\frac{2*3}{5}}),sin(\\frac{0}{10000^\\frac{2*4}{5}})$\n",
    "\n",
    "For the word two following will be the PE:\n",
    "\n",
    "$cos(\\frac{1}{10000^\\frac{2*0}{5}}),cos(\\frac{1}{10000^\\frac{2*1}{5}}),sin(\\frac{1}{10000^\\frac{2*2}{5}}),cos(\\frac{1}{10000^\\frac{2*3}{5}}),sin(\\frac{1}{10000^\\frac{2*4}{5}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "980000ea-b2da-4e59-a1b4-1ae5f68b3a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.dim())\n",
    "        print(self.pe.dim())\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6602a1-020b-4463-84e4-3752b2c2ff40",
   "metadata": {},
   "source": [
    "### Layer Normalization\n",
    "Its just batch norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44d97c23-5653-4826-84c9-72431af554bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, hidden_size)\n",
    "         # Keep the dimension for broadcasting\n",
    "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # Keep the dimension for broadcasting\n",
    "        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # eps is to prevent dividing by zero or when std is very small\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e80765-6f5d-44d9-8a65-6968bdb4f7c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Multihead attention block\n",
    "\n",
    "This is the crux of the model. We further break our Q,K,V matrices before self attention operation is done, so that we can look at interaction between words but for a **subset** of dimensions and not the whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4d15e68-dc31-4a45-ac77-c8c3e3243a51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Embedding vector size\n",
    "        self.h = h # Number of heads\n",
    "        # Make sure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "        return self.w_o(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6f9325-7c8b-4e6a-87b9-53e6dde59676",
   "metadata": {},
   "source": [
    "### Residual connections\n",
    "\n",
    "- We connect the embedding layer output to multihead attention output\n",
    "- We also connect the multihead attention output to the feedforward and batch norm output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37c31193-9cb9-4faa-859b-2311658673de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \n",
    "        def __init__(self, features: int, dropout: float) -> None:\n",
    "            super().__init__()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.norm = LayerNormalization(features)\n",
    "    \n",
    "        def forward(self, x, sublayer):\n",
    "            return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4437fa9-e3c5-42fa-9e73-3250922e1fc1",
   "metadata": {},
   "source": [
    "### Encoder Block\n",
    "\n",
    "- Combine embedding, positional embedding, multihead attention, batch norm and feed forward output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "991512bb-72f6-449f-8a79-12915adadc5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bd13b7-fc17-4ab3-bade-3809a27224cc",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "- Stack multiple encoder blocks one over the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5058b11-79b4-4ffd-ace5-752106f7ef44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc7d398-fbe4-466b-90e7-0f76135d276a",
   "metadata": {},
   "source": [
    "### Decoder block\n",
    "\n",
    "- Similar to encoder but also includes a `cross attention layer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7e6886d-cc48-4d0a-890c-d6b086fe609f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce5a424-777b-4a71-9c73-51e6c93dbb50",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "- Stack multiple decoders on top of each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c684eeb-cffd-4a93-8c62-7517069f73ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25577ead-9a9e-42b8-8680-e00129f54631",
   "metadata": {},
   "source": [
    "### Projection Layer\n",
    "\n",
    "- Decoder output is processed by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1812aada-cd81-41e0-adbf-6c4a8f300162",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4d4db4-25cb-4ce8-a6b9-71faf88b5804",
   "metadata": {},
   "source": [
    "### Transformer Model\n",
    "\n",
    "- Combine all the parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c998a346-e863-4d36-8350-c814ac6c7592",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        # (batch, seq_len, d_model)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.projection_layer(x)\n",
    "    \n",
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
    "    # Create the embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the positional encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "    \n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create the decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "    \n",
    "    # Create the encoder and decoder\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
    "    \n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "    \n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681456c7-36a9-4237-b0ea-3e31f53ea8fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3473a0-b89e-4345-afda-0f0e17cbb63a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
