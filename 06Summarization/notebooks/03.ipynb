{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "895b4d76-ce05-4f21-ba04-37ec291e83c4",
   "metadata": {},
   "source": [
    "--------------------------------------\n",
    "**Author**: Gunnvant\n",
    "\n",
    "**Description**: datasets preprocessing pipeline\n",
    "\n",
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfb1fccf-ba28-4150-b43c-66e994850afb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gunnvantsaini/miniforge3/envs/huggingface/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "ckpt = \"t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5203fef-f1c8-4e88-947b-6965181352fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 4505.16it/s]\n",
      "Extracting data files: 100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 290.12it/s]\n",
      "Generating train split: 2225 examples [00:00, 20077.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = load_dataset(\"csv\",data_files=\"../processed_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8708935-e84f-4f87-a30a-c1a55aeecd59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Text': 'UK economy facing \\'major risks\\'\\n\\nThe UK manufacturing sector will continue to face \"serious challenges\" over the next two years, the British Chamber of Commerce (BCC) has said.\\n\\nThe group\\'s quarterly survey of companies found exports had picked up in the last three months of 2004 to their best levels in eight years. The rise came despite exchange rates being cited as a major concern. However, the BCC found the whole UK economy still faced \"major risks\" and warned that growth is set to slow. It recently forecast economic growth will slow from more than 3% in 2004 to a little below 2.5% in both 2005 and 2006.\\n\\nManufacturers\\' domestic sales growth fell back slightly in the quarter, the survey of 5,196 firms found. Employment in manufacturing also fell and job expectations were at their lowest level for a year.\\n\\n\"Despite some positive news for the export sector, there are worrying signs for manufacturing,\" the BCC said. \"These results reinforce our concern over the sector\\'s persistent inability to sustain recovery.\" The outlook for the service sector was \"uncertain\" despite an increase in exports and orders over the quarter, the BCC noted.\\n\\nThe BCC found confidence increased in the quarter across both the manufacturing and service sectors although overall it failed to reach the levels at the start of 2004. The reduced threat of interest rate increases had contributed to improved confidence, it said. The Bank of England raised interest rates five times between November 2003 and August last year. But rates have been kept on hold since then amid signs of falling consumer confidence and a slowdown in output. \"The pressure on costs and margins, the relentless increase in regulations, and the threat of higher taxes remain serious problems,\" BCC director general David Frost said. \"While consumer spending is set to decelerate significantly over the next 12-18 months, it is unlikely that investment and exports will rise sufficiently strongly to pick up the slack.\"\\n',\n",
       " 'Summary': '\"Despite some positive news for the export sector, there are worrying signs for manufacturing,\" the BCC said.The BCC found confidence increased in the quarter across both the manufacturing and service sectors although overall it failed to reach the levels at the start of 2004.The outlook for the service sector was \"uncertain\" despite an increase in exports and orders over the quarter, the BCC noted.The UK manufacturing sector will continue to face \"serious challenges\" over the next two years, the British Chamber of Commerce (BCC) has said.However, the BCC found the whole UK economy still faced \"major risks\" and warned that growth is set to slow.The reduced threat of interest rate increases had contributed to improved confidence, it said.The rise came despite exchange rates being cited as a major concern.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5b44ff-7bf2-4057-a207-657ef2b6ccbc",
   "metadata": {},
   "source": [
    "### Preprocessing Steps\n",
    "\n",
    "- Use the data statistics to find the length of questions to be truncated at as `max_input_length`\n",
    "- Find out the `max_target_length` as well from the training data\n",
    "- Tokenize text and summaries. Create a new field `labels` as the input tokens from summary tokenization\n",
    "\n",
    "See [notebook](./02.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee0d72d9-8264-42f6-a38b-4c81d156a00b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_input_length = 723\n",
    "max_target_length = 313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8cef2e3-f5d6-430b-917d-a63c3c7a956c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab685eae-2ce1-4caa-9dd0-8e421bcdd269",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix = \"summarize: \"\n",
    "def preprocess(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"Text\"]]\n",
    "    model_inputs = tokenizer(inputs,max_length=max_input_length,\n",
    "        truncation=True,padding=True)\n",
    "    labels = tokenizer(examples['Summary'],max_length=max_target_length,truncation=True,padding=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb602fe3-dc8f-4462-b4c5-892ca79365f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96deae48-df75-4e58-ad39-fc8d066864df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(ckpt)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d263726-f14b-4f3b-9028-423638f92db7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0d437e-2aae-4c0d-8475-e43e45bb579f",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Use ROUGE score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efa5b588-6a84-41c4-94cf-4e6187eb120f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge_score = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10dc8dc9-2475-4e79-910e-605b30bf0f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def three_sentence_summary(text):\n",
    "    return \"\\n\".join(sent_tokenize(text)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "331bcc51-dea6-43e5-8fd5-2a4b0520157b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_baseline(dataset, metric):\n",
    "    summaries = [three_sentence_summary(text) for text in dataset[\"Text\"]]\n",
    "    return metric.compute(predictions=summaries, references=dataset[\"Summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3f7af6d-f1a3-4eb8-960b-98f0c1f0492e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score = evaluate_baseline(raw_dataset[\"train\"], rouge_score) ## change to validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d2249ed-1093-442a-a1bb-f65d5ecac757",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.44729677794630696,\n",
       " 'rouge2': 0.35164807984363006,\n",
       " 'rougeL': 0.3422369099766631,\n",
       " 'rougeLsum': 0.4052121651347588}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "163c6ec9-6198-4595-9504-bde8607866da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████| 2225/2225 [00:02<00:00, 846.34 examples/s]\n"
     ]
    }
   ],
   "source": [
    "### Create tokenized dataset\n",
    "tokenized_dataset = raw_dataset.map(preprocess,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48402127-c3ad-42ef-af16-2846158500b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns(['Text',\"Summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8426264d-0220-4f10-982a-5896da4472b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2225\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d87943-6f41-4472-a6e6-ca29dd3476be",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06bef596-faca-45c8-ad1c-5bd552c410e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "batch_size = 8\n",
    "num_train_epochs = 1\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(tokenized_dataset[\"train\"]) // batch_size\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"finetuned-amazon-en-es\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5.6e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=logging_steps,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "346d6738-1a51-4419-8ba7-14897a15b1e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Decode generated summaries into text\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Decode reference summaries into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    # Compute ROUGE scores\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Extract the median scores\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0152ae5-0fa4-448f-96aa-16e10188bf46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = [tokenized_dataset[\"train\"][i] for i in range(2)]\n",
    "#data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2bf704be-9d42-41ae-a92d-023e0622cd3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43d97372-8133-4d5e-aaaa-e7bbe638a2ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e58aaa-4705-4303-8c6f-a1784c987a2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b697bf-c13e-41b0-8326-5a5cf43bc6cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
